# ğŸš€ LangGraph-FastAPI Server

<div align="center">

![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

**A fully modular, production-ready FastAPI backend inspired by LangGraph server utilities**

*Provides asynchronous workflow execution, real-time streaming, persistent state, and a clean service-oriented architecture*

[Features](#-features) â€¢ [Architecture](#-architecture) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [API](#-api-endpoints) â€¢ [Roadmap](#-roadmap)

</div>

---

## ğŸ“‹ Overview

This project implements a **LangGraph-style execution server** using FastAPI. It is designed as a backend foundation for:

* ğŸ¤– Agentic AI systems
* ğŸ”„ Graph-based workflows
* ğŸ’¬ Stateful chat + streaming systems
* âš™ï¸ Async automation pipelines

The system separates concerns cleanly across API, orchestration, persistence, and streaming layers while remaining lightweight and extensible.

---

## âœ¨ Features

| Feature                     | Description                                 |
| --------------------------- | ------------------------------------------- |
| ğŸ”„ Async Workflow Execution | Background execution via asyncio task queue |
| ğŸ“Š Run Management           | Create, track, list, and update runs        |
| ğŸ’¾ Persistent Artifacts     | JSON/file outputs saved per run             |
| ğŸ“ Checkpointing            | Intermediate execution snapshots            |
| ğŸ—‚ï¸ State Management        | Persisted workflow/chat state               |
| ğŸŒ WebSocket Streaming      | Real-time status and token updates          |
| ğŸ§  Chat Memory              | Threaded, stateful conversations            |
| ğŸ—ï¸ Modular Architecture    | API â†’ Services â†’ Utils â†’ Models             |

---

## ğŸ›ï¸ Architecture

### High-Level System Architecture

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffedce', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#f4f4f4'}}}%%
graph TD
    Client[Client: Frontend / Swagger / curl]
    DB[(Database)]
    Queue[Async Task Queue]
    Artifacts[Artifact Store]

    subgraph "API Service (FastAPI)"
        Router[HTTP Router]
        Pydantic[Pydantic Validation]
        Manager[Run / Chat Manager]
        WS[WebSocket Server]
    end

    Client -->|POST /api/runs or /api/chat| Router
    Router --> Pydantic
    Pydantic --> Manager
    Manager -->|Persist state| DB
    Manager -->|Schedule job| Queue

    Queue -->|Execute workflow / LLM| WS
    WS -.->|Live stream events/tokens| Client

    Queue -->|Save result| Artifacts
    Queue -->|Final status update| DB
```

---

### Workflow & Chat Execution Flow

```
USER ACTION
   â”‚
   â–¼
HTTP API (FastAPI)
   â”‚
   â”œâ”€ Validate request (Pydantic)
   â”œâ”€ Persist state / message
   â”œâ”€ Enqueue async job
   â”‚
   â–¼
ASYNC TASK QUEUE
   â”‚
   â”œâ”€ Execute workflow / LLM
   â”œâ”€ Save checkpoints & state
   â”œâ”€ Emit streaming events
   â”‚
   â–¼
WEBSOCKET STREAM
   â”‚
   â”œâ”€ started
   â”œâ”€ token / node_update
   â”œâ”€ completed
   â”‚
   â–¼
FRONTEND UI
```

---

## ğŸ“ Project Structure

```
langgraph-server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI initialization + .env loader
â”‚   â”œâ”€â”€ database.py                # Async DB setup (SQLite + aiosqlite)
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”œâ”€â”€ auth.py                    # Authentication utilities
â”‚   â”œâ”€â”€ init_db.py                 # Database initialization script
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py                # Router aggregator
â”‚   â”‚   â”œâ”€â”€ runs.py                # Run streaming endpoints
â”‚   â”‚   â”œâ”€â”€ stream.py              # WebSocket stream handler
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â””â”€â”€ chat.py            # Chat API (message history, send)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ run.py                 # Run workflow model
â”‚   â”‚   â”œâ”€â”€ chat.py                # Chat aggregate model
â”‚   â”‚   â”œâ”€â”€ chat_message.py        # Individual chat messages
â”‚   â”‚   â”œâ”€â”€ chat_thread.py         # Chat thread/conversation
â”‚   â”‚   â””â”€â”€ user_model.py          # User authentication model
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ run.py                 # Pydantic validation models
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ run_manager.py         # CRUD ops for runs
â”‚   â”‚   â”œâ”€â”€ workflow_service.py    # Main workflow logic
â”‚   â”‚   â”œâ”€â”€ chat_service.py        # Chat message processing + LLM streaming
â”‚   â”‚   â”œâ”€â”€ chat_memory.py         # Chat history persistence
â”‚   â”‚   â”œâ”€â”€ artifact_store.py      # Save result artifacts
â”‚   â”‚   â”œâ”€â”€ checkpoint_store.py    # Save execution checkpoints
â”‚   â”‚   â””â”€â”€ state_services.py      # Maintain run state
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ base.py                # Base LLM interface
â”‚   â”‚   â”œâ”€â”€ mock.py                # Groq LLM implementation
â”‚   â”‚   â””â”€â”€ provider.py            # LLM provider singleton
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ task_queue.py          # Async task queue
â”‚   â”‚   â”œâ”€â”€ stream_manager.py      # WebSocket connection manager
â”‚   â”‚   â”œâ”€â”€ logger.py              # Logging utilities
â”‚   â”‚   â”œâ”€â”€ db.py                  # Database utilities
â”‚   â”‚   â””â”€â”€ redis_manager.py       # Redis connection manager
â”‚   â”‚
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ logging.py             # Request logging
â”‚       â””â”€â”€ rate_limit.py          # Rate limiting middleware
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fastgraph.db               # SQLite database
â”‚   â”œâ”€â”€ artifacts/                 # Final output files (JSON results)
â”‚   â”œâ”€â”€ checkpoints/               # Execution snapshots
â”‚   â””â”€â”€ states/                    # State store
â”‚
â”œâ”€â”€ frontend/                      # React 19 dashboard (Vite + Tailwind)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Main chat interface
â”‚   â”‚   â”œâ”€â”€ App.css                # Custom styles
â”‚   â”‚   â””â”€â”€ main.jsx               # React entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ .env                           # Environment variables (API keys)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ test_run.py                    # Workflow test script
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### Backend Setup

```bash
git clone https://github.com/OrydleAI/Langgraph-FastAPI-Server.git
cd Langgraph-FastAPI-Server/langgraph-server

python -m venv venv
source venv/bin/activate  # Windows: venv\\Scripts\\activate

pip install -r requirements.txt
python -m app.init_db
uvicorn app.main:app --reload --port 8000
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

---

## ğŸ“– Usage

| Service      | URL                                                      |
| ------------ | -------------------------------------------------------- |
| Frontend UI  | [http://localhost:5173](http://localhost:5173)           |
| Swagger Docs | [http://localhost:8000/docs](http://localhost:8000/docs) |
| WebSocket    | ws://localhost:8000/api/ws/{thread_id}                   |

### Create a Workflow Run

```http
POST /api/runs/
{
  "name": "demo",
  "payload": { "input": "Hello" }
}
```

### Send Chat Message

```http
POST /api/chat/{thread_id}/message
{
  "message": "Hello agent"
}
```

---

## ğŸŒ API Endpoints

| Method | Endpoint                      | Description         |
| ------ | ----------------------------- | ------------------- |
| POST   | /api/runs/                    | Create workflow run |
| GET    | /api/runs/                    | List runs           |
| POST   | /api/chat/{thread_id}/message | Send chat message   |
| GET    | /api/chat/{thread_id}/history | Chat history        |
| WS     | /api/ws/{thread_id}           | Streaming updates   |

---

## ğŸ› ï¸ Tech Stack

**Backend**

* Python 3.12+
* FastAPI
* SQLAlchemy (Async)
* aiosqlite
* Pydantic v2
* asyncio + WebSockets

**Frontend**

* React 19
* Vite
* TailwindCSS

---

## ğŸ—ºï¸ Roadmap

* âœ… Async workflow execution
* âœ… WebSocket streaming
* âœ… Chat with LLM (Groq)
* âœ… Persistent state & memory
* ğŸ”„ Tool calling & function execution
* ğŸ”„ Multi-agent orchestration
* ğŸ”„ Token-level streaming UI improvements
* ğŸ”„ Persistent queues (Redis/Celery)
* ğŸ”„ Auth & API keys

---

## ğŸ‘¨â€ğŸ’» Author

**Rohit Ranjan Kumar**  
GitHub: [https://github.com/OrydleAI](https://github.com/OrydleAI)

---

Made with â¤ï¸ by [OrydleAI](https://github.com/OrydleAI)

</div>
